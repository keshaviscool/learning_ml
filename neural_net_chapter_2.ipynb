{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5d23de",
   "metadata": {},
   "source": [
    "# The exploding Vanishing gradient problem\n",
    "\n",
    "Fanin = no of inputs coming in the layer\n",
    "Fanout = no of neurons in that layer\n",
    "## glorot --> tanh, logistic, softmax\n",
    "## He initialisaiion --> Relu\n",
    "## LeCUN --> SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59ed3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense, built=False>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default, keras uses Glorot Intialsiation, you can change it like this\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37a246f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense_1, built=False>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want He initialization with a uniform distribution but based on fanavg rather\n",
    "# than fanin, you can use the VarianceScaling initializer like this:\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9376557",
   "metadata": {},
   "source": [
    "One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sig‐ moid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural net‐ works—in particular, the ReLU activation function, mostly because it does not satu‐ rate for positive values (and because it is fast to compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48ee0a",
   "metadata": {},
   "source": [
    "## Dying Relu (as weighted sum becomes negative and does not change with gradient descent as slope becomes zero as well)\n",
    "\n",
    "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neu‐ ron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045aab5",
   "metadata": {},
   "source": [
    "To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. This function is defined as LeakyReLUα(z) = max(αz, z) (see Figure 11-2). The hyperparameter α defines how much the function “leaks”: it is the slope of the function for z < 0 and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to even‐ tually wake up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864b8b0",
   "metadata": {},
   "source": [
    "1. Leaky relu\n",
    "2. Randomized leaky relu (RRelu)\n",
    "3. Parametric leaky relu (PRelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98114daf",
   "metadata": {},
   "source": [
    "# exponential linear unit (ELU) # new activation function\n",
    "\n",
    "## MUCH BETTER PERFORMANCE THAN RELUU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31baea",
   "metadata": {},
   "source": [
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb389f4",
   "metadata": {},
   "source": [
    "Then, a 2017 paper7 by Günter Klambauer et al. introduced the Scaled ELU (SELU) activation function: as its name suggests, it is a scaled variant of the ELU activation function. The authors showed that if you build a neural network composed exclu‐ sively of a stack of dense layers, and if all hidden layers use the SELU activation func‐ tion, then the network will self-normalize: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (espe‐ cially deep ones). There are, however, a few conditions for self-normalization to hap‐ pen (see the paper for the mathematical justification):\n",
    "• The input features must be standardized (mean 0 and standard deviation 1).\n",
    "• Every hidden layer’s weights must be initialized with LeCun normal initialization.\n",
    "In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
    "• The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks (see Chap‐ ter 15) or networks with skip connections (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904097e1",
   "metadata": {},
   "source": [
    "### whcih actiocation functiont theh ?\n",
    "So, which activation function should you use for the hidden layers of your deep neural networks? Although your mileage will vary, in general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network’s architecture prevents it from self- normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another hyperparameter, you may use the default α values used by Keras (e.g., 0.3 for leaky ReLU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as RReLU if your network is overfitting or PReLU if you have a huge training set. That said, because ReLU is the most used activation function (by far), many libraries and hardware accelerators provide ReLU-specific optimizations; therefore, if speed is your priority, ReLU might still be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c29fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keshavsharma/Downloads/DATA SCIENCE LESSGO/chai_aur_code_numpy/venv/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# To use the leaky ReLU activation function, create a LeakyReLU layer and add it to your model just after the layer you want to apply it to:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2) # adds leaky relu to above layer\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca55713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PRELU activation\n",
    "# To use the leaky ReLU activation function, create a LeakyReLU layer and add it to your model just after the layer you want to apply it to:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU() # adds leaky relu to above layer\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# selu activation \n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efd6c6",
   "metadata": {},
   "source": [
    "## No need to use StandardScaler, use batch normalization layer instead\n",
    "\n",
    "f you add a BN layer as the very first layer of your neural network, you do not need to standardize your train‐ ing set (e.g., using a StandardScaler); the BN layer will do it for you (well, approxi‐ mately, since it only looks at one batch at a time, and it can also rescale and shift each input feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keshavsharma/Downloads/DATA SCIENCE LESSGO/chai_aur_code_numpy/venv/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#### Batch normalisation\n",
    "\n",
    "# appling Batch normalization in keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # output layer does not require batch normalization\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c98845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary() # BN layer adds four parameters per input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c778480",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient clipppping\n",
    "\n",
    "## clips all gradient between a certain limit\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1) # clips gradient from -1 to 1.. this is to preecnt exploding gradient problem\n",
    "model.compile(optimizer=optimizer, loss=\"mse\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2ce575",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tip: use clipnorm, instead of clipvalue, to retain orientation\n",
    "\n",
    "### but this may result in a very small value if the gradient vector has small value in one dirn and one value in another, \n",
    "## so, you can use a combination of both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80dd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_layer = model.layers[1] # batch normalisation layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8598d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### reusing pretrained layers\n",
    "\n",
    "# using layers of \"model\" on a new model\n",
    "\n",
    "model_layers_without_output_layer = model.layers[:-1]\n",
    "new_model = keras.models.Sequential(model_layers_without_output_layer)\n",
    "new_model.add(keras.layers.Dense(1, activation=\"sigmoid\")) # output layer for out new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75203b",
   "metadata": {},
   "source": [
    "Note that model_A and model_B_on_A now share some layers. When you train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone model_A before you reuse its layers. To do this, you clone model A’s architecture with clone_model(), then copy its weights (since clone_model() does not clone the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b87376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_clone = keras.models.clone_model(new_model)\n",
    "new_model_clone.set_weights(new_model.get_weights()) # now you can train the new model clone without altering the original model layers parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f732171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making reused layers untrainable\n",
    "\n",
    "for layer in new_model_clone.layers:\n",
    "    layer.trainable= False\n",
    "\n",
    "# compile model whenever you change its layers settings (like if any of them is trainable or not trainable etc)\n",
    "new_model_clone.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59746487",
   "metadata": {},
   "source": [
    "so,\n",
    "1. we first trained the model with reused layers, with setting all reused layers as untrainable.. this way the output layer weights of the new_model_clone will adjust themselves with the reused layers... lets say for 4 epochs\n",
    "\n",
    "2. then we set all layers as trainable, nowww we train the rest of the epochs on the it (of course after compiling it.. )\n",
    "\n",
    "3. bonus: also reduce the learning rate so we do not completely ruin the reused layers weights\n",
    "\n",
    "# AND BOOM, WE GET A GREAT FUcKING ACCURACY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f76b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "#                                validation_data=(X_valid_B, y_valid_B)) \n",
    "# for layer in model_B_on_A.layers[:-1]: \n",
    "#         layer.trainable = True \n",
    "# optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2 model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "#                          metrics=[\"accuracy\"])\n",
    "#     history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "#                                validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38c0b4",
   "metadata": {},
   "source": [
    "## optimizers i have completed and made a notes of them when to use what optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e58d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing Momenturm with SGD\n",
    "\n",
    "optimizer = keras.optimizers.SGD(momentum=0.9, learning_rate=0.001) # putting beta as 0.9 in momentum with SGD optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9349f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemeting NAG (Nestreov accelareated gradient)\n",
    "\n",
    "# NAG is generally faster than regular momentum optimization. To use it, simply set nesterov=True when creating the SGD optimizer:\n",
    "\n",
    "optimizer = keras.optimizers.SGD(momentum=0.9, learning_rate=0.001, nesterov=True) # NAG implemented\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "861836d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing ADAGRAD (for scarce datasets) # dynamic learning rate\n",
    "\n",
    "## Keras has an Adagrad optimizer, you should not use it to train deep neu‐ ral networks (it may be efficient for simpler tasks such as Linear Regression, though).\n",
    "\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001) # no hyperparameters to tune\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea77e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMS PROP (BEST LOVELY) for scarce datasets (ADAGRAD KE CONECEPTS KE UPAR BUILT HEI.. DEAL WITH THE VANISHING LEARNING RATE PROBLEM)\n",
    "# it does so by using exponential decay\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(rho=0.9) # rho is beta here # jitni badi rho(beta) ki value, utna hi kam importance to purani values(gradients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa64af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM OPTIMIZER (LOVELY BEST BEST BESTT!!!!! IMPLEMENTING EXPOENETIAL DECAY LEARNING RATE AND MOMMENTUM IN A SINGLE OPTIMIZER)\n",
    "\n",
    "adam_opti = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# beta1 is for momentum\n",
    "# beta2 is for decaing gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3d6c2",
   "metadata": {},
   "source": [
    "# Nadam is basically adam with Nestorov trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146dada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
