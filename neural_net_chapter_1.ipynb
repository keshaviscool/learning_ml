{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79230eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df2f4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashon_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2243740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashon_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d455d0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
       "          1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
       "          0,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
       "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
       "         10,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
       "         72,  15],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
       "        172,  66],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
       "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
       "        229,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
       "        173,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
       "        202,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
       "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
       "        209,  52],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
       "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
       "        167,  56],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "         92,   0],\n",
       "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
       "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
       "         77,   0],\n",
       "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
       "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
       "        159,   0],\n",
       "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
       "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
       "        215,   0],\n",
       "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
       "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
       "        246,   0],\n",
       "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
       "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
       "        225,   0],\n",
       "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
       "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
       "        229,  29],\n",
       "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
       "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
       "        230,  67],\n",
       "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
       "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
       "        206, 115],\n",
       "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
       "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
       "        210,  92],\n",
       "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
       "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
       "        170,   0],\n",
       "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
       "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b2250e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e66589db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additionally, since we are going to train the neural network using Gradient Descent, we must scale the input features\n",
    "\n",
    "## we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 (this also converts them to floats)\n",
    "\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255, X_train_full[5000:] / 255\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                   \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e9a8bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shirt'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[1234]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f28b7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keshavsharma/Downloads/DATA SCIENCE LESSGO/chai_aur_code_numpy/venv/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Now let’s build the neural network! Here is a classification MLP with two hidden layers:\n",
    "\n",
    "# MLP --> Multi layer Percepton\n",
    "\n",
    "model = keras.models.Sequential() # keras Sequential API\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) # input layer,  It is a Flatten layer whose role is to convert each input image into a 1D array:\n",
    "# model.add(keras.layers.InputLayer(input_shape=[28,28]))  can also use it for input layer\n",
    "\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) # hidden layer 300 neuron \n",
    "model.add(keras.layers.Dense(100, activation=\"relu\")) # hidden layer  100 neuron\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) # output layer, after softmax applied to it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46d5b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "]) # better way of doing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "156a3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.name = \"Keshav Sharma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f84e347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Keshav Sharma\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Keshav Sharma\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87d09c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Flatten name=flatten_3, built=True>,\n",
       " <Dense name=dense_11, built=True>,\n",
       " <Dense name=dense_12, built=True>,\n",
       " <Dense name=dense_13, built=True>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "981bbf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_11'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a509d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29ffeb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3185b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (kernel is another name for the matrix of connection\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead.\n",
    "\n",
    "# If we were doing binary classification (with one or more binary labels), then we would use the \"sigmoid\" (i.e., logistic) activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14515e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'Keshav Sharma' is not a valid root scope name. A root scope name has to match the following pattern: ^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/DATA SCIENCE LESSGO/chai_aur_code_numpy/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'Keshav Sharma' is not a valid root scope name. A root scope name has to match the following pattern: ^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257256e6",
   "metadata": {},
   "source": [
    "Instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation. For example, validation_split=0.1 tells Keras to use the last 10% of the data (before shuffling) for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5e9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 'auto', 'epochs': 30, 'steps': 1719}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c3f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106955b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.7612545490264893,\n",
       "  0.8295817971229553,\n",
       "  0.8435817956924438,\n",
       "  0.8547818064689636,\n",
       "  0.859854519367218,\n",
       "  0.8658545613288879,\n",
       "  0.8702727556228638,\n",
       "  0.8723636269569397,\n",
       "  0.876945436000824,\n",
       "  0.8802363872528076,\n",
       "  0.8837817907333374,\n",
       "  0.8855090737342834,\n",
       "  0.8885999917984009,\n",
       "  0.8903999924659729,\n",
       "  0.8934545516967773,\n",
       "  0.8953272700309753,\n",
       "  0.8961636424064636,\n",
       "  0.8991090655326843,\n",
       "  0.9004908800125122,\n",
       "  0.9028545618057251,\n",
       "  0.9042181968688965,\n",
       "  0.9057818055152893,\n",
       "  0.9070363640785217,\n",
       "  0.9102363586425781,\n",
       "  0.9107818007469177,\n",
       "  0.9109636545181274,\n",
       "  0.91356360912323,\n",
       "  0.916345477104187,\n",
       "  0.9174000024795532,\n",
       "  0.9171817898750305],\n",
       " 'loss': [0.7181106805801392,\n",
       "  0.4887324273586273,\n",
       "  0.44380155205726624,\n",
       "  0.4158899188041687,\n",
       "  0.3959178030490875,\n",
       "  0.3790874779224396,\n",
       "  0.3661527931690216,\n",
       "  0.35416123270988464,\n",
       "  0.3442692160606384,\n",
       "  0.33388322591781616,\n",
       "  0.32575860619544983,\n",
       "  0.3180176913738251,\n",
       "  0.3105897307395935,\n",
       "  0.30396968126296997,\n",
       "  0.29673418402671814,\n",
       "  0.2914596498012543,\n",
       "  0.2862705588340759,\n",
       "  0.2803114652633667,\n",
       "  0.2759174704551697,\n",
       "  0.2708044648170471,\n",
       "  0.266446977853775,\n",
       "  0.26166123151779175,\n",
       "  0.25642791390419006,\n",
       "  0.25235116481781006,\n",
       "  0.24819797277450562,\n",
       "  0.2445383369922638,\n",
       "  0.24013692140579224,\n",
       "  0.23602768778800964,\n",
       "  0.23167215287685394,\n",
       "  0.22826771438121796],\n",
       " 'val_accuracy': [0.7853999733924866,\n",
       "  0.8515999913215637,\n",
       "  0.852400004863739,\n",
       "  0.8593999743461609,\n",
       "  0.8682000041007996,\n",
       "  0.8679999709129333,\n",
       "  0.8694000244140625,\n",
       "  0.8748000264167786,\n",
       "  0.8809999823570251,\n",
       "  0.8744000196456909,\n",
       "  0.8740000128746033,\n",
       "  0.8723999857902527,\n",
       "  0.883400022983551,\n",
       "  0.8812000155448914,\n",
       "  0.8873999714851379,\n",
       "  0.8804000020027161,\n",
       "  0.8885999917984009,\n",
       "  0.8781999945640564,\n",
       "  0.8920000195503235,\n",
       "  0.8895999789237976,\n",
       "  0.8903999924659729,\n",
       "  0.8894000053405762,\n",
       "  0.8859999775886536,\n",
       "  0.8870000243186951,\n",
       "  0.8902000188827515,\n",
       "  0.890999972820282,\n",
       "  0.8907999992370605,\n",
       "  0.8916000127792358,\n",
       "  0.8888000249862671,\n",
       "  0.8934000134468079],\n",
       " 'val_loss': [0.5734629034996033,\n",
       "  0.4373597204685211,\n",
       "  0.4355165660381317,\n",
       "  0.3996412754058838,\n",
       "  0.384395569562912,\n",
       "  0.3772652745246887,\n",
       "  0.37103763222694397,\n",
       "  0.3557226061820984,\n",
       "  0.34245774149894714,\n",
       "  0.36077091097831726,\n",
       "  0.35117337107658386,\n",
       "  0.35548895597457886,\n",
       "  0.32974380254745483,\n",
       "  0.3253106474876404,\n",
       "  0.32744139432907104,\n",
       "  0.33984583616256714,\n",
       "  0.31441664695739746,\n",
       "  0.333504855632782,\n",
       "  0.31297388672828674,\n",
       "  0.30680692195892334,\n",
       "  0.307265967130661,\n",
       "  0.3088285028934479,\n",
       "  0.3122461140155792,\n",
       "  0.3095117509365082,\n",
       "  0.30996185541152954,\n",
       "  0.2994612157344818,\n",
       "  0.30608442425727844,\n",
       "  0.29960212111473083,\n",
       "  0.31061041355133057,\n",
       "  0.2925584316253662]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d38cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1] plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9169 - loss: 0.2294 - val_accuracy: 0.8932 - val_loss: 0.2979\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9213 - loss: 0.2179 - val_accuracy: 0.8902 - val_loss: 0.3020\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.2182 - val_accuracy: 0.8972 - val_loss: 0.2873\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9228 - loss: 0.2135 - val_accuracy: 0.8936 - val_loss: 0.2920\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9248 - loss: 0.2092 - val_accuracy: 0.8936 - val_loss: 0.2922\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9271 - loss: 0.2050 - val_accuracy: 0.8930 - val_loss: 0.2994\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9263 - loss: 0.2055 - val_accuracy: 0.8962 - val_loss: 0.3015\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9289 - loss: 0.2021 - val_accuracy: 0.8924 - val_loss: 0.2938\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9289 - loss: 0.1988 - val_accuracy: 0.8922 - val_loss: 0.3059\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9304 - loss: 0.1927 - val_accuracy: 0.8946 - val_loss: 0.2885\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9293 - loss: 0.1966 - val_accuracy: 0.8938 - val_loss: 0.2934\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9328 - loss: 0.1866 - val_accuracy: 0.8874 - val_loss: 0.3012\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9361 - loss: 0.1820 - val_accuracy: 0.8952 - val_loss: 0.2893\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9376 - loss: 0.1795 - val_accuracy: 0.8960 - val_loss: 0.2923\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9359 - loss: 0.1814 - val_accuracy: 0.8940 - val_loss: 0.2993\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9385 - loss: 0.1738 - val_accuracy: 0.8934 - val_loss: 0.2951\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9386 - loss: 0.1739 - val_accuracy: 0.8954 - val_loss: 0.2940\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9389 - loss: 0.1707 - val_accuracy: 0.8838 - val_loss: 0.3267\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9398 - loss: 0.1698 - val_accuracy: 0.8886 - val_loss: 0.3166\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9419 - loss: 0.1661 - val_accuracy: 0.8994 - val_loss: 0.2859\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9442 - loss: 0.1618 - val_accuracy: 0.8944 - val_loss: 0.2985\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9431 - loss: 0.1595 - val_accuracy: 0.8924 - val_loss: 0.3105\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9457 - loss: 0.1547 - val_accuracy: 0.8896 - val_loss: 0.3101\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9453 - loss: 0.1531 - val_accuracy: 0.8942 - val_loss: 0.3077\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9460 - loss: 0.1516 - val_accuracy: 0.8916 - val_loss: 0.2985\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9470 - loss: 0.1502 - val_accuracy: 0.8926 - val_loss: 0.3149\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9476 - loss: 0.1459 - val_accuracy: 0.8914 - val_loss: 0.3019\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9491 - loss: 0.1435 - val_accuracy: 0.8964 - val_loss: 0.2959\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1440 - val_accuracy: 0.8890 - val_loss: 0.3205\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9513 - loss: 0.1405 - val_accuracy: 0.8974 - val_loss: 0.2964\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid)) # continues from where it left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8587 - loss: 67.1447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[69.48444366455078, 0.8567000031471252]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## evaludating the model on test set\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daadba",
   "metadata": {},
   "source": [
    "Remember to resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the generalization error will be too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea25b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.09, 0.  , 0.49, 0.  , 0.41, 0.  , 0.  , 0.  ],\n",
       "       [0.9 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.96, 0.  , 0.04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting\n",
    "X_new = X_train[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d7bd8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Regression MLP Using the Sequential API\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "acef3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test,  y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "         X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dab1f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1aac3",
   "metadata": {},
   "source": [
    "The main differences are the fact that the output layer has a single neuron (since we only want to predict a sin‐ gle value) and uses no activation function, and the loss function is the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "10df9aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m  1/363\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 126ms/step - loss: 6.3825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keshavsharma/Downloads/DATA SCIENCE LESSGO/chai_aur_code_numpy/venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 1.3952 - val_loss: 0.9857\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.7117 - val_loss: 0.5198\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 0.5229 - val_loss: 0.4807\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.5015 - val_loss: 0.4652\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.4766 - val_loss: 0.4497\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.4551 - val_loss: 0.4468\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.4630 - val_loss: 0.4267\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.4360 - val_loss: 0.4199\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.4287 - val_loss: 0.4127\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.4342 - val_loss: 0.4109\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.4386 - val_loss: 0.4031\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.4128 - val_loss: 0.3963\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.4171 - val_loss: 0.3914\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.4083 - val_loss: 0.4009\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4237 - val_loss: 0.3866\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.4050 - val_loss: 0.3838\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.3865 - val_loss: 0.3776\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.3992 - val_loss: 0.3743\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.3923 - val_loss: 0.3713\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.4045 - val_loss: 0.3796\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.3882\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:] ), # skipping rows) # 30 neurons,\n",
    "    keras.layers.Dense(1) # output layer, only one neuron, no activation function (regression model)\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\") # ok\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# making predictions\n",
    "X_new = X_test[:3] # pretend these are new instances y_pred = model.predict(X_new)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9e347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2105587],\n",
       "       [1.370321 ],\n",
       "       [1.8295108]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c796d4",
   "metadata": {},
   "source": [
    "## Functional API (Wide and DEEP Non Sequential Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2764ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8dca287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 2.2915 - val_loss: 0.5276\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.5781 - val_loss: 0.4967\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.5336 - val_loss: 0.4616\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.5411 - val_loss: 0.4555\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.4816 - val_loss: 0.4474\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.4505 - val_loss: 0.9311\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.5581 - val_loss: 0.4296\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4377 - val_loss: 0.4190\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4474 - val_loss: 0.4116\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.5326 - val_loss: 0.4104\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.4129 - val_loss: 0.3882\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.4442 - val_loss: 0.3820\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.4039 - val_loss: 0.3807\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.3884 - val_loss: 0.3714\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.3918 - val_loss: 0.3672\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4729 - val_loss: 0.3728\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4153 - val_loss: 0.3754\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 1.1601 - val_loss: 0.3968\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.4324 - val_loss: 0.3763\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.4184 - val_loss: 0.3741\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - loss: 0.3802\n"
     ]
    }
   ],
   "source": [
    "func_model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\", metrics=[\"\"]) # ok\n",
    "history = func_model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = func_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e5b37",
   "metadata": {},
   "source": [
    "For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bf688e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hid1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hid2 = keras.layers.Dense(30, activation=\"relu\")(hid1)\n",
    "concat_layer = keras.layers.concatenate([input_A, hid2])\n",
    "output_layer = keras.layers.Dense(1, name=\"output_layer\")(concat_layer)\n",
    "\n",
    "model_ = keras.models.Model(inputs=[input_A, input_B], outputs=[output_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde7cff",
   "metadata": {},
   "source": [
    "Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "096998df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3)) ## lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "80e7adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1fe85d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.5244 - val_loss: 0.4868\n",
      "Epoch 2/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.4903 - val_loss: 0.4839\n",
      "Epoch 3/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 0.5034 - val_loss: 0.4805\n",
      "Epoch 4/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.4839 - val_loss: 0.4815\n",
      "Epoch 5/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.5194 - val_loss: 0.4765\n",
      "Epoch 6/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 0.4978 - val_loss: 0.4745\n",
      "Epoch 7/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.4771 - val_loss: 0.4731\n",
      "Epoch 8/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.5160 - val_loss: 0.4705\n",
      "Epoch 9/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 0.5021 - val_loss: 0.4687\n",
      "Epoch 10/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.4878 - val_loss: 0.4680\n",
      "Epoch 11/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.4991 - val_loss: 0.4654\n",
      "Epoch 12/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.4813 - val_loss: 0.4638\n",
      "Epoch 13/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4798 - val_loss: 0.4630\n",
      "Epoch 14/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.4881 - val_loss: 0.4602\n",
      "Epoch 15/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.4879 - val_loss: 0.4586\n",
      "Epoch 16/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.4885 - val_loss: 0.4575\n",
      "Epoch 17/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.4762 - val_loss: 0.4546\n",
      "Epoch 18/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.4931 - val_loss: 0.4537\n",
      "Epoch 19/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.4781 - val_loss: 0.4522\n",
      "Epoch 20/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4590 - val_loss: 0.4516\n",
      "Epoch 21/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.4640 - val_loss: 0.4496\n",
      "Epoch 22/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.4662 - val_loss: 0.4499\n",
      "Epoch 23/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.4675 - val_loss: 0.4471\n",
      "Epoch 24/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.4548 - val_loss: 0.4471\n",
      "Epoch 25/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.4643 - val_loss: 0.4451\n",
      "Epoch 26/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.4816 - val_loss: 0.4438\n",
      "Epoch 27/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4578 - val_loss: 0.4431\n",
      "Epoch 28/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.4585 - val_loss: 0.4417\n",
      "Epoch 29/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.4455 - val_loss: 0.4410\n",
      "Epoch 30/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.4655 - val_loss: 0.4412\n",
      "Epoch 31/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.4608 - val_loss: 0.4392\n",
      "Epoch 32/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.4548 - val_loss: 0.4379\n",
      "Epoch 33/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4603 - val_loss: 0.4363\n",
      "Epoch 34/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.4606 - val_loss: 0.4353\n",
      "Epoch 35/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.4576 - val_loss: 0.4347\n",
      "Epoch 36/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.4421 - val_loss: 0.4335\n",
      "Epoch 37/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.4407 - val_loss: 0.4326\n",
      "Epoch 38/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4483 - val_loss: 0.4331\n",
      "Epoch 39/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.4621 - val_loss: 0.4318\n",
      "Epoch 40/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.4662 - val_loss: 0.4291\n",
      "Epoch 41/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.4416 - val_loss: 0.4295\n",
      "Epoch 42/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.4606 - val_loss: 0.4272\n",
      "Epoch 43/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.4322 - val_loss: 0.4269\n",
      "Epoch 44/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.4460 - val_loss: 0.4253\n",
      "Epoch 45/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4468 - val_loss: 0.4252\n",
      "Epoch 46/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.4260 - val_loss: 0.4235\n",
      "Epoch 47/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.4479 - val_loss: 0.4249\n",
      "Epoch 48/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.4238 - val_loss: 0.4219\n",
      "Epoch 49/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.4266 - val_loss: 0.4211\n",
      "Epoch 50/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.4477 - val_loss: 0.4199\n",
      "Epoch 51/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.4395 - val_loss: 0.4196\n",
      "Epoch 52/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4404 - val_loss: 0.4180\n",
      "Epoch 53/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.4279 - val_loss: 0.4189\n",
      "Epoch 54/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.4395 - val_loss: 0.4165\n",
      "Epoch 55/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.4375 - val_loss: 0.4157\n",
      "Epoch 56/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.4278 - val_loss: 0.4158\n",
      "Epoch 57/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.4240 - val_loss: 0.4138\n",
      "Epoch 58/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4424 - val_loss: 0.4146\n",
      "Epoch 59/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.4313 - val_loss: 0.4135\n",
      "Epoch 60/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.4243 - val_loss: 0.4116\n",
      "Epoch 61/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.4240 - val_loss: 0.4100\n",
      "Epoch 62/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.4317 - val_loss: 0.4106\n",
      "Epoch 63/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.4427 - val_loss: 0.4084\n",
      "Epoch 64/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.4394 - val_loss: 0.4084\n",
      "Epoch 65/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.4332 - val_loss: 0.4074\n",
      "Epoch 66/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.4212 - val_loss: 0.4069\n",
      "Epoch 67/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.4369 - val_loss: 0.4065\n",
      "Epoch 68/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.4215 - val_loss: 0.4057\n",
      "Epoch 69/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.4297 - val_loss: 0.4041\n",
      "Epoch 70/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.4102 - val_loss: 0.4038\n",
      "Epoch 71/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.4400 - val_loss: 0.4020\n",
      "Epoch 72/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.4202 - val_loss: 0.4027\n",
      "Epoch 73/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4346 - val_loss: 0.4006\n",
      "Epoch 74/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.4166 - val_loss: 0.4013\n",
      "Epoch 75/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 0.4284 - val_loss: 0.3997\n",
      "Epoch 76/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.4239 - val_loss: 0.3983\n",
      "Epoch 77/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.4149 - val_loss: 0.3982\n",
      "Epoch 78/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.4279 - val_loss: 0.3974\n",
      "Epoch 79/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.4182 - val_loss: 0.3956\n",
      "Epoch 80/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.4138 - val_loss: 0.3967\n",
      "Epoch 81/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.4216 - val_loss: 0.3942\n",
      "Epoch 82/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.4278 - val_loss: 0.3949\n",
      "Epoch 83/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.4010 - val_loss: 0.3938\n",
      "Epoch 84/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.4132 - val_loss: 0.3922\n",
      "Epoch 85/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.4096 - val_loss: 0.3926\n",
      "Epoch 86/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.4351 - val_loss: 0.3920\n",
      "Epoch 87/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.4049 - val_loss: 0.3925\n",
      "Epoch 88/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.4154 - val_loss: 0.3908\n",
      "Epoch 89/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.4114 - val_loss: 0.3911\n",
      "Epoch 90/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.4042 - val_loss: 0.3888\n",
      "Epoch 91/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.4117 - val_loss: 0.3875\n",
      "Epoch 92/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.4114 - val_loss: 0.3876\n",
      "Epoch 93/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.4057 - val_loss: 0.3869\n",
      "Epoch 94/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.4077 - val_loss: 0.3855\n",
      "Epoch 95/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.4051 - val_loss: 0.3859\n",
      "Epoch 96/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.4001 - val_loss: 0.3854\n",
      "Epoch 97/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.4042 - val_loss: 0.3849\n",
      "Epoch 98/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.4016 - val_loss: 0.3844\n",
      "Epoch 99/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.4135 - val_loss: 0.3829\n",
      "Epoch 100/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.4259 - val_loss: 0.3818\n"
     ]
    }
   ],
   "source": [
    "history = model_.fit((X_train_A, X_train_B), y_train, epochs=100, validation_data=((X_valid_A, X_valid_B), y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6ed418c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also add extra outputs if you want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [...] # Same as above, up to the main output layer\n",
    "# output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "# aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "# model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "# # We care much more about the main output than about the auxiliary output (as it is just used for regulari‐ zation),\n",
    "# # so we want to give the main output’s loss a much greater weight\n",
    "\n",
    "# model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n",
    "\n",
    "\n",
    "# history = model.fit(\n",
    "#          [X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "#          validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "#  total_loss, main_loss, aux_loss = model.evaluate(\n",
    "#         [X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "\n",
    "# y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a72dd",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models\n",
    "\n",
    "\n",
    "Both the Sequential API and the Functional API are declarative: you start by declar‐ ing which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages: the model can easily be saved, cloned, and shared; its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early (i.e., before any data ever goes through the model). It’s also fairly easy to debug, since the whole model is a static graph of layers. But the flip side is just that: it’s static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more impera‐ tive programming style, the Subclassing API is for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06a6b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ef8e3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model): # subclassing keras.Model\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "573919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2a87e",
   "metadata": {},
   "source": [
    "this is for extra flexibility and you can define inputs later on this pre defined architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "54a7cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "## you can save your model by calling\n",
    "\n",
    "model_.save(\"best_model.h5\") #HDF5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "loaded_model = keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586567da",
   "metadata": {},
   "source": [
    "## very important information\n",
    "\n",
    "But what if training lasts several hours? This is quite common, especially when train‐ ing on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes. But how can you tell the fit() method to save checkpoints? Use callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ae61206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keshavsharma/Downloads/DATA SCIENCE LESSGO/chai_aur_code_numpy/venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:] ), # skipping rows) # 30 neurons,\n",
    "    keras.layers.Dense(1) # output layer, only one neuron, no activation function (regression model)\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a8526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 1.2919\n",
      "Epoch 2/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 4.4291\n",
      "Epoch 3/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.4992\n",
      "Epoch 4/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429us/step - loss: 0.4737\n",
      "Epoch 5/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442us/step - loss: 0.4411\n",
      "Epoch 6/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414us/step - loss: 0.4270\n",
      "Epoch 7/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 0.4263\n",
      "Epoch 8/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.4129\n",
      "Epoch 9/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413us/step - loss: 0.4074\n",
      "Epoch 10/10\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - loss: 0.4153\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.keras\") # save it with this name \n",
    "# ModelCheckpoint saves your model at end each of epoch\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfeae4e",
   "metadata": {},
   "source": [
    "Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set. The following code is a simple way to implement early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c3a5f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4142 - val_loss: 0.3747\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 0.4003 - val_loss: 0.3738\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.4047 - val_loss: 0.3735\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.3887 - val_loss: 0.3651\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.4108 - val_loss: 0.3647\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.3781 - val_loss: 0.3684\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.3884 - val_loss: 0.3582\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.3920 - val_loss: 0.3612\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3705 - val_loss: 0.3542\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 0.3747 - val_loss: 0.3527\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.3710 - val_loss: 0.3564\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.3737 - val_loss: 0.3487\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.3618 - val_loss: 0.3561\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.3690 - val_loss: 0.3469\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.3570 - val_loss: 0.3435\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.3791 - val_loss: 0.3424\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.3455 - val_loss: 0.3429\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.3719 - val_loss: 0.3421\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.3576 - val_loss: 0.3403\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3549 - val_loss: 0.3536\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_best_model.keras\", save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, callbacks=[checkpoint_cb])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "319a9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_best_model = keras.models.load_model(\"my_best_model.keras\") # loads the model with lowest Validation Loss Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "97bd1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.3502\n"
     ]
    }
   ],
   "source": [
    "l = my_best_model.evaluate(X_test, y_test) # good model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84d9fc",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the EarlyStopping call‐ back. It will interrupt training when it measures no progress on the validation set for \n",
    "a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. You can combine both callbacks to save checkpoints of your model (in case your computer crashes) and interrupt training early when there is no more progress (to avoid wasting time and resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "328e30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3599 - val_loss: 0.3375\n",
      "Epoch 2/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.3515 - val_loss: 0.3353\n",
      "Epoch 3/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.3493 - val_loss: 0.3378\n",
      "Epoch 4/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.3611 - val_loss: 0.3404\n",
      "Epoch 5/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.3448 - val_loss: 0.3423\n",
      "Epoch 6/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.3531 - val_loss: 0.3365\n",
      "Epoch 7/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.3582 - val_loss: 0.3425\n",
      "Epoch 8/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.3367 - val_loss: 0.3293\n",
      "Epoch 9/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.3619 - val_loss: 0.3314\n",
      "Epoch 10/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.3340 - val_loss: 0.3290\n",
      "Epoch 11/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.3423 - val_loss: 0.3279\n",
      "Epoch 12/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.3473 - val_loss: 0.3273\n",
      "Epoch 13/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.3563 - val_loss: 0.3285\n",
      "Epoch 14/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.3388 - val_loss: 0.3253\n",
      "Epoch 15/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.3229 - val_loss: 0.3419\n",
      "Epoch 16/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.3368 - val_loss: 0.3254\n",
      "Epoch 17/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.3420 - val_loss: 0.3328\n",
      "Epoch 18/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.3479 - val_loss: 0.3228\n",
      "Epoch 19/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.3394 - val_loss: 0.3245\n",
      "Epoch 20/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.3310 - val_loss: 0.3217\n",
      "Epoch 21/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.3323 - val_loss: 0.3236\n",
      "Epoch 22/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.3516 - val_loss: 0.3237\n",
      "Epoch 23/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.3464 - val_loss: 0.3190\n",
      "Epoch 24/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.3327 - val_loss: 0.3217\n",
      "Epoch 25/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.3345 - val_loss: 0.3175\n",
      "Epoch 26/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.3218 - val_loss: 0.3210\n",
      "Epoch 27/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.3298 - val_loss: 0.3196\n",
      "Epoch 28/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.3408 - val_loss: 0.3169\n",
      "Epoch 29/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.3251 - val_loss: 0.3179\n",
      "Epoch 30/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.3291 - val_loss: 0.3159\n",
      "Epoch 31/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.3285 - val_loss: 0.3154\n",
      "Epoch 32/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.3229 - val_loss: 0.3188\n",
      "Epoch 33/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.3323 - val_loss: 0.3244\n",
      "Epoch 34/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.3189 - val_loss: 0.3186\n",
      "Epoch 35/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.3433 - val_loss: 0.3166\n",
      "Epoch 36/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.3205 - val_loss: 0.3143\n",
      "Epoch 37/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.3204 - val_loss: 0.3352\n",
      "Epoch 38/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.3211 - val_loss: 0.3165\n",
      "Epoch 39/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.3291 - val_loss: 0.3128\n",
      "Epoch 40/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.3262 - val_loss: 0.3128\n",
      "Epoch 41/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.3257 - val_loss: 0.3125\n",
      "Epoch 42/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.3154 - val_loss: 0.3216\n",
      "Epoch 43/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 0.3248 - val_loss: 0.3140\n",
      "Epoch 44/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.3202 - val_loss: 0.3122\n",
      "Epoch 45/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.3389 - val_loss: 0.3179\n",
      "Epoch 46/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.3318 - val_loss: 0.3136\n",
      "Epoch 47/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.3240 - val_loss: 0.3110\n",
      "Epoch 48/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.3237 - val_loss: 0.3230\n",
      "Epoch 49/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.3218 - val_loss: 0.3121\n",
      "Epoch 50/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.3328 - val_loss: 0.3164\n",
      "Epoch 51/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.3163 - val_loss: 0.3094\n",
      "Epoch 52/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.3127 - val_loss: 0.3066\n",
      "Epoch 53/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.3058 - val_loss: 0.3137\n",
      "Epoch 54/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.3234 - val_loss: 0.3143\n",
      "Epoch 55/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.3245 - val_loss: 0.3094\n",
      "Epoch 56/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.3210 - val_loss: 0.3114\n",
      "Epoch 57/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.3027 - val_loss: 0.3215\n",
      "Epoch 58/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.3301 - val_loss: 0.3053\n",
      "Epoch 59/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.3087 - val_loss: 0.3107\n",
      "Epoch 60/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.3244 - val_loss: 0.3069\n",
      "Epoch 61/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.3240 - val_loss: 0.3050\n",
      "Epoch 62/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.3255 - val_loss: 0.3176\n",
      "Epoch 63/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.3240 - val_loss: 0.3151\n",
      "Epoch 64/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.3241 - val_loss: 0.3043\n",
      "Epoch 65/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.3337 - val_loss: 0.3446\n",
      "Epoch 66/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.4748 - val_loss: 0.3090\n",
      "Epoch 67/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.3437 - val_loss: 0.3375\n",
      "Epoch 68/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.3395 - val_loss: 0.8028\n",
      "Epoch 69/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.3737 - val_loss: 0.3152\n",
      "Epoch 70/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.3396 - val_loss: 0.3094\n",
      "Epoch 71/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.3193 - val_loss: 0.3069\n",
      "Epoch 72/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.3074 - val_loss: 0.3107\n",
      "Epoch 73/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.3232 - val_loss: 0.3126\n",
      "Epoch 74/100\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.3235 - val_loss: 0.3234\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a74f2c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.3276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29941442608833313"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no need to load the model again, the early stopping will automatically restore the best weights to the model\n",
    "model.evaluate(X_test, y_test) ### bestest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54dcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## writing custom callback\n",
    "\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    # you can implement on_train_begin(), on_train_end(),\n",
    "    #  on_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end()\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
